{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aa613dc",
   "metadata": {},
   "source": [
    "# Step 1: Setting Up the Environment\n",
    "\n",
    "# Step 2: Importing Necessary Libraries\n",
    "\n",
    "We will use TensorFlow and PyTorch, popular AI frameworks used for developing machine learning models. \n",
    "\n",
    "These frameworks provide a comprehensive set of tools that enable developers to easily create and deploy ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f09787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59924b5b",
   "metadata": {},
   "source": [
    "# Step 3: Gathering and Preprocessing Data\n",
    "\n",
    "The first step in building a language model is to gather and preprocess the data. \n",
    "\n",
    "The data for a language model is typically a large corpus of text. \n",
    "\n",
    "For example, you could use a book, a collection of articles, or any other large text file.\n",
    "\n",
    "https://www.mltut.com/how-to-build-generative-ai-model/ \n",
    "\n",
    "\n",
    "\n",
    "Once you have your text data, you'll need to preprocess it. This typically involves:\n",
    "\n",
    "* Tokenization: Splitting the text into individual words or tokens.\n",
    "* Lowercasing: Converting all the text to lowercase to ensure the model doesn't treat the same word in different cases as different words.\n",
    "* Removing punctuation and non-alphanumeric characters: This simplifies the model's input space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0092f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb40995",
   "metadata": {},
   "source": [
    "# Step 4: Building the Model\n",
    "\n",
    "We will use a Recurrent Neural Network (RNN) for our language model. \n",
    "\n",
    "RNNs are great for generating sequences, like sentences or melodies5.\n",
    "\n",
    "Here's a simple example of how you might define an RNN in PyTorch\n",
    "\n",
    "https://coda.io/@peter-sigurdson/building-a-simple-ai-generative-language-model-in-python#_lu891"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04f50b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f5f400",
   "metadata": {},
   "source": [
    "torch is imported to ensure we have access to all necessary PyTorch functions and classes.\n",
    "\n",
    "torch.nn is aliased as nn for ease of use.\n",
    "\n",
    "The RNNModel class extends nn.Module, which is the base class for all neural network modules in PyTorch.\n",
    "\n",
    "The forward method is where the input tensor x goes through the layers of the network.\n",
    "\n",
    "To actually run this, you'll need to install PyTorch if it's \n",
    "not already available in your session.\n",
    "\n",
    "!pip install torch\n",
    "\n",
    "After importing the necessary libraries and defining your model, \n",
    "you'll be ready to instantiate the RNNModel class and use it for whatever task you have in mind, such as text generation or another sequence modeling task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb7aa6c",
   "metadata": {},
   "source": [
    "1. First class line declares a class named RNNModel that inherits from nn.Module, which is a base class for all PyTorch neural network modules.\n",
    "2. The __init__ method initializes the RNNModel. It takes four parameters:\n",
    "\n",
    "- vocab_size: The size of the vocabulary (number of unique words).\n",
    "- embed_size: The size of the word embeddings.\n",
    "- hidden_size: The number of features in the hidden state of the RNN.\n",
    "- num_layers: The number of RNN layers.\n",
    "\n",
    "It then calls the constructor of the parent class (nn.Module) using super()\n",
    "\n",
    "3. MOdel components are:\n",
    "- self.embed: An embedding layer that converts input indices (word IDs) into dense vectors of fixed size (embed_size).\n",
    "- self.rnn: An RNN layer with num_layers layers, taking input of size embed_size and producing a hidden state of size hidden_size.\n",
    "- self.linear: A linear layer that maps the RNN hidden state to the output vocabulary size (vocab_size).\n",
    "\n",
    "4. The forward method defines the forward pass of the model. It takes two parameters:\n",
    "\n",
    "- x: Input sequence (word indices).\n",
    "- h: Initial hidden state of the RNN.\n",
    "\n",
    "The forward pass involves the following steps:\n",
    "\n",
    "Embedding the input sequence (x) using the embedding layer.\n",
    "Passing the embedded sequence through the RNN, producing an output tensor (out) and an updated hidden state (h).\n",
    "Applying the linear layer to the RNN output to get the final prediction.\n",
    "The method returns the output tensor and the updated hidden state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18496fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        x = self.embed(x)\n",
    "        out, h = self.rnn(x, h)\n",
    "        out = self.linear(out)\n",
    "        return out, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddfd408",
   "metadata": {},
   "source": [
    "In summary, this code defines a basic RNN model for language modeling or sequence prediction tasks, where the goal is to predict the next word in a sequence given the previous words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234b4af2",
   "metadata": {},
   "source": [
    "# Step 5: Training the Model\n",
    "\n",
    "Training involves feeding your preprocessed data into the model, calculating the error of the model's predictions, and updating the model's parameters to reduce this error. \n",
    "\n",
    "This process is repeated for a number of iterations or epochs5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9523b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, epochs, lr):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        hidden = None\n",
    "        for x, y in data:\n",
    "            optimizer.zero_grad()\n",
    "            outputs, hidden = model(x, hidden)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6363cb",
   "metadata": {},
   "source": [
    "# Step 6: Using the Model for Inference [Doing the next token generation]\n",
    "\n",
    "Once the model is trained, you can use it to generate new text. \n",
    "\n",
    "This involves:\n",
    "\n",
    "\n",
    "Providing the model with a seed sequence\n",
    "\n",
    "Having the model make a prediction for the next word\n",
    "\n",
    "Adding the predicted word to the sequence\n",
    "\n",
    "Repeating this process for as many words as you want to generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a126636",
   "metadata": {},
   "source": [
    "Below code says:-\n",
    "    \n",
    "1. Function signature\n",
    "\n",
    "  The function is defined with three parameters:\n",
    "\n",
    "- model: The PyTorch model used for text generation.\n",
    "- seed_text: A starting sequence of text from which the generation begins.\n",
    "- num_words: The number of words to generate.\n",
    "\n",
    "2. Set Model to Evaluation Mode:\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "This line sets the model to evaluation mode. In PyTorch, this is important because it disables certain operations like dropout, which are typically used during training but not during evaluation or generation.\n",
    "\n",
    "3. Initialize Text:\n",
    "\n",
    "    text = seed_text\n",
    "    \n",
    "The text variable is initialized with the provided seed_text. This will be the starting point for text generation.\n",
    "\n",
    "4. Generate Text:\n",
    "\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        x = torch.tensor([text[-1]])\n",
    "        output, _ = model(x, None)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        text.append(predicted.item())\n",
    "        \n",
    "        \n",
    "Inside the loop, a tensor x is created with the last word of the current text (text[-1]). This tensor is then passed to the model.\n",
    "The model's forward pass is executed (model(x, None)), and the output tensor (output) is obtained. The second return value (_) represents the hidden state, which is not used here (None is passed).\n",
    "\n",
    "The torch.max function is used to get the index of the word with the highest probability in the output distribution. This index (predicted) is added to the text as the next predicted word.\n",
    "\n",
    "5. Return Generated Text:\n",
    "\n",
    "    return text\n",
    "    \n",
    "After generating the specified number of words, the function returns the complete generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8883c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_text, num_words):\n",
    "    model.eval()\n",
    "    text = seed_text\n",
    "    for _ in range(num_words):\n",
    "        x = torch.tensor([text[-1]])\n",
    "        output, _ = model(x, None)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        text.append(predicted.item())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f248e5",
   "metadata": {},
   "source": [
    "In summary, this function takes a trained model, a seed text, and a number of words to generate. It utilizes the model to predict the next word in the sequence iteratively, updating the text with each prediction, and finally returns the generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9278ad30",
   "metadata": {},
   "source": [
    "# Step 7: Interacting with the Model\n",
    "\n",
    "You will interact with your trained model by providing it with a seed sequence {prompt engineering} and having it generate a response. This can be done in a loop to simulate a conversation with the model5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b22efba",
   "metadata": {},
   "source": [
    "Simple learning guide on creating and training a generative language model using Python, focusing on Transformers-based library called GPT-2. This model has been well noted for its ability in generating coherent and contextually relevant sentences based on a given prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34728165",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\monika201103\\anaconda3\\lib\\site-packages (4.29.2)\n",
      "Requirement already satisfied: requests in c:\\users\\monika201103\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\monika201103\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\monika201103\\anaconda3\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\monika201103\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\monika201103\\anaconda3\\lib\\site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\monika201103\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\monika201103\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\monika201103\\anaconda3\\lib\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: filelock in c:\\users\\monika201103\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\monika201103\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\monika201103\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\monika201103\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\monika201103\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\monika201103\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\monika201103\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\monika201103\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\monika201103\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\monika201103\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\monika201103\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\monika201103\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\monika201103\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\monika201103\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7f3fd8",
   "metadata": {},
   "source": [
    "# Step 8: Importing the Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdb04788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399772a18f194631adae3a0342724ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\monika201103\\Anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\monika201103\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c525ece94094849a9ef2871833206b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe88537",
   "metadata": {},
   "source": [
    "# Step 9: Encoding and Decoding Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed44d17",
   "metadata": {},
   "source": [
    "These two functions are related to text encoding and decoding using the **Hugging Face Transformers library**. This library is commonly used for natural language processing tasks, including pre-trained models like BERT, GPT, etc. The provided functions are specifically designed to work with a tokenizer provided by this library.\n",
    "\n",
    "Here's a breakdown of each function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d454e7",
   "metadata": {},
   "source": [
    "**encode function:**\n",
    "\n",
    "def encode(prompt):\n",
    "    return tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "This function takes a prompt as input, which is a piece of text or a sentence.\n",
    "It uses the Hugging Face tokenizer to encode the text. The encode method of the tokenizer converts the input text into a sequence of numerical IDs (usually representing subword tokens).\n",
    "\n",
    "The return_tensors=\"pt\" argument specifies that the function should return a PyTorch tensor. This is useful when working with PyTorch-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0aaec77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(prompt):\n",
    "    return tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "def decode(encoded_prompt):\n",
    "    return tokenizer.decode(encoded_prompt[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52bd709",
   "metadata": {},
   "source": [
    "**decode function:\n",
    "\n",
    "def decode(encoded_prompt):\n",
    "    return tokenizer.decode(encoded_prompt[0], skip_special_tokens=True)\n",
    "\n",
    "This function takes an encoded_prompt as input, which is the result of the encoding process (a tensor of numerical IDs).\n",
    "It uses the Hugging Face tokenizer again, this time with the decode method. This method converts the numerical IDs back into human-readable text.\n",
    "The skip_special_tokens=True argument instructs the tokenizer to exclude any special tokens (e.g., [CLS], [SEP]) that might have been added during the encoding process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a241917",
   "metadata": {},
   "source": [
    "In summary, these functions provide a convenient way to encode and decode text using the Hugging Face Transformers library's tokenizer. They are particularly useful when working with models that require input in a numerical format (encoded) and when you want to convert the model's output back into human-readable text (decoded)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87859444",
   "metadata": {},
   "source": [
    "# Step 10: Running The Model\n",
    "\n",
    "Now for the fun part: generating text!\n",
    "\n",
    "Inputs and outputs to the GPT2 model are all sequences of integers. We can encode our input prompt, generate a response, and then decode this response to get our output message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e67c0363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are you feeling today? Can you smell? Do you feel good? Are you able to speak? Have you ever had to work after work that made you cringe? And that's just one part of the journey that we are taking today.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_prompt = \"How are you feeling today?\"\n",
    "\n",
    "input_prompt_encoded = encode(input_prompt)\n",
    "output = model.generate(input_prompt_encoded, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2, do_sample=True)\n",
    "output_message = decode(output)\n",
    "\n",
    "print(output_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac705b",
   "metadata": {},
   "source": [
    "# Step 10: Create a conversational model\n",
    "\n",
    "**A conversational model** is a type of natural language processing (NLP) model designed to understand and generate human-like responses in the context of a conversation. These models are often built using techniques from machine learning and deep learning and can be used for various applications, including chatbots, virtual assistants, and other interactive systems.\n",
    "\n",
    "For creating a conversational model, one needs to append the prompt to all previous dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "335f92f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm an AI model. Hello, how are you?  It sounds so familiar. If I look for those words in my head, they are.  \n",
      "I don't know what that means. I have my thoughts, if I recall accurately.   \n",
      "This is a big one. It's one of those things that I think is so central to human experience where we are all human. \n",
      "What?   I will be getting more into this soon.\n",
      "How\n"
     ]
    }
   ],
   "source": [
    "# conversation history\n",
    "history_encoded = tokenizer.encode(\"Hello, I'm an AI model. \", return_tensors=\"pt\")\n",
    "\n",
    "# user input\n",
    "user_input_encoded = tokenizer.encode(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "\n",
    "# append the new user input tokens to the chat history\n",
    "history_with_user_input_encoded = torch.cat([history_encoded, user_input_encoded], dim=-1)\n",
    "\n",
    "# generate a response\n",
    "output = model.generate(history_with_user_input_encoded, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, do_sample=True)\n",
    "\n",
    "history_with_reply_encoded = output\n",
    "\n",
    "# Print message\n",
    "output_message = decode(history_with_reply_encoded)\n",
    "print(output_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dac0747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
